{"componentChunkName":"component---src-templates-post-template-jsx","path":"/blog/generate-intuitive-locustio-reports-with-jtl-reporter/","result":{"data":{"site":{"siteMetadata":{"title":"ludeknovy.tech","subtitle":"Thoughts on testing and test automation","copyright":"© All rights reserved.","author":{"name":"","twitter":"#"},"disqusShortname":"","url":"https://www.ludeknovy.tech"}},"markdownRemark":{"id":"f263e650-3403-5ed1-9b95-5b06164cb067","html":"<p>Locust.io is without any doubts an excellent performance testing tool. But by default, it lacks one very important feature - better reporting capabilities. Although out of the box its UI provides charts and label statistics, the charts are not persistent. The label statistics can be downloaded in CSV during or after the test run. The issue with end result CSV statistics is that you lose a significant amount of information - the test progress. If there was any considerable performance drop or error rate spike, you won’t know at what time this had happened. Thus, you won’t be able to correlate it to backend service metrics. At last but not least manual storing and comparing CSV files is counter-intuitive and error-prone.</p>\n<p>\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/379362c3c7f43835fd64de339b56eadc/5a791/report_locust.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 960px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 141.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAYAAABh2p9gAAAACXBIWXMAAAsSAAALEgHS3X78AAADE0lEQVRIx51V2a7TMBDtFwISILEIIRbBAx/HK0K88RF8wUVsbZrEdrxmOGfspOltBYJIJzOe8TKbx7t7D5/Iq7fv5Onz1/Lo2Qu5i/Gd+4/l7oPHcudvwDyuf/byDda+wviR7H7tO/lxdPKtAw5G+mMv0zSJsU6cm8SCkrfKTys/gi/eyvsvWT5/+iL9h4/y8zDIbhidTC5JiEVSLtIZLwcbZJ5nGX2QKSaRWXCIx5wIeeUjeH59j4PGSdJohHvt+sGKd0FCCHUCNrvZD7pwgCWHYdTNjbUyYhE/Ywz4UQrk2UKfE8/E5gYb4udD1FMd4L2XhNMjLCMCdLSMdPJBaUz5JI/g27x+MHTZaly+f/8hNzdfcUov1z7rnHTH4+rq7Y9e0Nsdd2Xg6Q4tTTnDgnQdsCSlS73KoFMLuSs3G1qs5nIL84bO13WlFMkwpFloVUClz8hq9kAQlyq1CVkEb0CXsepzlRG5ZN1DNzwyKZ4lEcSESV0IKTYklE0Qj7g50lRplZPHpkDnjCZytTBpHJLGMGqMTmBtptTQ+FUHnhtPKZwsZCCXryAOjMVCI2qTNaewAGrP4xblpt/OXWNIl227YmHNYsNSh1uwPm95cVaHNcsjrs2op6zZ/EecuUyBFieE5T8RT0kxOriWkNiKmMWeNuN4xjPjWStldZmusjQ0HhssY95hvcctfltdpQlWzrgctjaHpbBzLhegbmpNQ7NaLvWxNY+1Djs0VSJrneULLIuv6Rac3WUWtZ6G3GiPY9YUgms1r1Bdmdu8qktNd9ZtPOLjPCo+VLgG8gZy6sx0GpMyieNUdaQMy2ohu3XUVs+TykWN6RtwUXt1HuUeVzBs7/K2ns6uVRvzQGIrY4dZxkmzXGqWa7cJ9RFig70ScD4N9WFKTXZOfXseVguP/SCHQ6cba5Mot11Wpzeymrxl3LlZ9qZZyN+Ajr08jb65t4BjHrRSJhDNpD5avs7DurzcZb6lbE9dd5T9/oDJTot4eSo5XkAZHzGLJ5U8nw1SHhbzfIphaE/mKUZ/xrUi9619/QY2oWrm/BFcHAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"Performance report\"\n        title=\"Performance report\"\n        src=\"/static/379362c3c7f43835fd64de339b56eadc/d9199/report_locust.png\"\n        srcset=\"/static/379362c3c7f43835fd64de339b56eadc/8ff5a/report_locust.png 240w,\n/static/379362c3c7f43835fd64de339b56eadc/e85cb/report_locust.png 480w,\n/static/379362c3c7f43835fd64de339b56eadc/d9199/report_locust.png 960w,\n/static/379362c3c7f43835fd64de339b56eadc/5a791/report_locust.png 1248w\"\n        sizes=\"(max-width: 960px) 100vw, 960px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    </p>\n<p>As I described in another article I had a very similar issue with JMeter and <a href=\"https://www.ludeknovy.tech/blog/generate-intuitive-jmeter-reports-with-jtlreporter-and-taurus/\">I solved it by implementing Jtl Reporter</a>. By using <a href=\"https://github.com/ludeknovy/jtl-reporter\">Jtl Reporter</a> you will get neat performance reports from your tests with the possibility for convenient comparison, test reports management and easily share it with anyone on your team (now you can get rid of HTML reports).</p>\n<p>Luckily Locust.io is very hackable by its nature, and extending it is easy thanks to its well-documented event API. I have created a listener that collects data during the performance and saves them into JTL file which conforms JMeter format. Here are two caveats regarding the listener  - it was developed for distributed mode and some of the data (latency and connection time) are not available when compared to JMeter output as Locust.io does not provide them.</p>\n<h2>How do I get started?</h2>\n<p>No worries, its fairly easy and straightforward, there is just a couple of things you need to do:</p>\n<ol>\n<li>Deploy Jtl Reporter. Just follow <a href=\"https://github.com/ludeknovy/jtl-reporter#installation-steps\">the installation steps</a> in the readme of the project repository.</li>\n<li>Create a new project and scenario in your Jtl Reporter instance.</li>\n<li>Add <a href=\"https://github.com/ludeknovy/python-test-stack-demo/blob/master/jtl_listener.py\">jtl_listener.py</a> into the root folder with your Locust.io test scripts.</li>\n<li>\n<p>Add JtlListener class into your test scripts. This is done by adding these lines to your User class:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@events<span class=\"token punctuation\">.</span>init<span class=\"token punctuation\">.</span>add_listener</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">on_locust_init</span><span class=\"token punctuation\">(</span>environment<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>_kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\nJtlListener<span class=\"token punctuation\">(</span>env<span class=\"token operator\">=</span>environment<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>Upload JTL file into Jtl Reporter.</li>\n</ol>\n<p>I have created a <a href=\"https://github.com/ludeknovy/python-test-stack-demo\">demo repository</a> with a simple test, so you can check it out and try to generate JTL file on your own. Again, simply follow the steps in the readme to install everything and how to run the tests. Do not forget to start PetStore API, otherwise, you will get but errors.</p>\n<p>Let the test do its job for a while. You should now see JTL file in the <code class=\"language-text\">logs</code> folder in the root folder of the repository.  Take it and upload it to the Jtl Reporter to your project’s scenario and wait for the report. Once processed you will get a report like on the image above.</p>\n<p>In an upcoming blog post we will look at how to upload the results to the Jtl Reporter automatically.</p>\n<p>Happy testing!</p>","fields":{"tagSlugs":["/tags/performance-testing/","/tags/locust-io/"],"slug":"/blog/generate-intuitive-locustio-reports-with-jtl-reporter/"},"frontmatter":{"title":"Generate intuitive Locust.io reports with Jtl Reporter","tags":["performance testing","locust.io"],"date":"2020-09-29T02:46:37.121Z","description":"Tutorial on how to create Locust.io performance reports."}}},"pageContext":{"slug":"/blog/generate-intuitive-locustio-reports-with-jtl-reporter/"}},"staticQueryHashes":[]}